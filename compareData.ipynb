{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests pandas matplotlib unidecode nltk numpy==1.19.3 spacy==2.2.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def createPie(data: dict):\n",
    "    saliences = [x*100 for x in data['salience']]\n",
    "    wedges = plt.pie(\n",
    "        saliences, labels=data['types'], shadow=True, autopct='%1.1f%%', startangle=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def createTable(data: list):\n",
    "    types = []\n",
    "    salience = []\n",
    "    entities = []\n",
    "    count = []\n",
    "    for x in data:\n",
    "        types.append(x['type'])\n",
    "        salience.append(x['salience'])\n",
    "        entities.append(x['entities'])\n",
    "        count.append(len(x['entities']))\n",
    "        print('Entities for Type '+x['type']+' is ')\n",
    "        print(x['entities'])\n",
    "    stemdf = pd.DataFrame({'type': types, 'salience': salience,\n",
    "                           'entitiesCount': count, 'entities': entities})\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Tabular format'+'\\033[0m'+'\\n')\n",
    "    print(stemdf)\n",
    "    return {\"salience\": salience, \"types\": types}\n",
    "def filterStopWords(data: string):\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    # all_stopwords.remove(\"not\")\n",
    "    text_tokens = word_tokenize(data)\n",
    "    tokens_without_sw = [\n",
    "        word for word in text_tokens if not word in all_stopwords]\n",
    "    return \" \".join([token for token in tokens_without_sw])\n",
    "\n",
    "\n",
    "def lemmatization(data: string):\n",
    "    doc = sp(data)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "def filterSpecialChar(data: list):\n",
    "    alphaNumericData = []\n",
    "    for x in data:\n",
    "        alphaNumericData.append(x.translate(\n",
    "            str.maketrans('', '', string.punctuation)))\n",
    "    return alphaNumericData\n",
    "\n",
    "\n",
    "def preprocessData(data: list):\n",
    "    data = [''.join(x) for x in data]  # convert to list of string\n",
    "    data = list(dict.fromkeys(data))  # remove duplicate values\n",
    "    data = list(filter(None, data))   # remove empty values\n",
    "    data = filterSpecialChar(data)  # remove special characters\n",
    "    data = [re.sub(\"(<.*?>)\", \"\", word) for word in data]  # remove html markup\n",
    "    # Convert accented characters to ASCII characters\n",
    "    data = [unidecode.unidecode(word) for word in data]\n",
    "    data = [word.strip() for word in data]  # remove trailing space\n",
    "    data = [decontracted(word)\n",
    "            for word in data]  # handled contractions\n",
    "    data = [filterStopWords(word) for word in data]  # filter StopWords\n",
    "    data = [lemmatization(word) for word in data]  # Lemmatization\n",
    "    data = appendIfAbsent('.', data)        # add . in end of line\n",
    "    data = [word.lower() for word in data]  # lower case\n",
    "    data = [word.replace('-pron-', '') for word in data]\n",
    "    data = [word.strip() for word in data]  # remove trailing space\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def appendIfAbsent(s, data):\n",
    "    fixedData = []\n",
    "    for x in data:\n",
    "        fixedData.append(\n",
    "            x.lower() + s) if not x.endswith(s) else fixedData.append(x.lower())\n",
    "    return fixedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "data = [\"The vet was very good\", \" My cat did not like the vet, but my dog did\", \" I do not like vets, they are too expensive\", \"I love my vet\", \" I love my dog\", \" I like brown cats best\", \" I like brown dogs\", \" The cat jump through a window and injured itself\", \"I enjoyed the experience\", \" I would recommend my vet\", \" The vet was good, but I had to wait too long and my cat\", \" I have farm animals and the vet does not support them\", \" I like horses\", \" My cat is my best friend and I give him the best\", \"I valued the transaction\", \" I thought the service was over priced\", \" I do not think the vet likes cats\", \" My cat does not like vets\", \" I had to wait to long\", \" It was very expensive\", \" I was not told how much it would cost\", \"I have three cats and a rabbit and take them all to the same vet\",\n",
    "        \" My cat was very sick and the vet was fantastic\", \"I wish the vet was quicker\", \" I had to wait to long and would not recommend this vet\", \"I wanted to by insurance for my pet, but they would not\", \" My dog misbehaved at the vet\", \" I adopted my dog from this vet\", \" I need a microchip scanned\", \" The vet was very sensitive to my needs at a challenging time\", \" The vet lost my cat and I was devastated\", \" I had a budgie, but it escaped at the vet\", \" The medicine was very expensive\", \" I need to take the cat back to the vet many times\", \" I liked the vet\", \" I miss my original vet\", \" I wish my doctor was as good as my vet\", \" Why is the vet more expensive than my doctor? I am happy with the treatment given by my vet\", \" My cat caught fleas at the vet\", \" She needed a vaccination\"]\n",
    "\n",
    "url = 'https://text-classification-engine-rest-dot-feijoa8-dev.ts.r.appspot.com/api/v1/text/classify/entity-sentiments'\n",
    "header = {'Content-Type': 'application/json',\n",
    "          'industryVertical': 'testing', 'Accept': 'application/json'}\n",
    "\n",
    "\n",
    "requestBody = {\"data\": data}\n",
    "response = requests.post(url, json=requestBody, headers=header)\n",
    "response_dict = json.loads(response.text)\n",
    "\n",
    "preProcesseddata = preprocessData(data)\n",
    "requestBodyPreprocess = {\"data\": preProcesseddata}\n",
    "responsePrepProcess = requests.post(\n",
    "    url, headers=header, json=requestBodyPreprocess)\n",
    "response_dict_preprocess = json.loads(responsePrepProcess.text)\n",
    "\n",
    "print('\\033[1m'+'Without data preprocessing'+'\\033[0m'+'\\n')\n",
    "print('\\n')\n",
    "value = createTable(response_dict)\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Pie'+'\\033[0m'+'\\n')\n",
    "createPie(value)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'With data preprocessing'+'\\033[0m'+'\\n')\n",
    "print('\\n')\n",
    "preProcessedValue = createTable(response_dict_preprocess)\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Pie'+'\\033[0m'+'\\n')\n",
    "createPie(preProcessedValue)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
